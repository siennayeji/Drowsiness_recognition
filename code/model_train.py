# -*- coding: utf-8 -*-
import os, json
from glob import glob
from PIL import Image, UnidentifiedImageError
from collections import Counter

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split, Subset, WeightedRandomSampler
from torch.utils.data.dataloader import default_collate
from torchvision import transforms, models

# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
BATCH_SIZE   = 8
EPOCHS       = 10
SEQ_LEN      = 5
NUM_CLASSES  = 2
LR           = 1e-4
TEMPERATURE  = 4.0
ALPHA        = 0.7  # KD vs CE ê°€ì¤‘ì¹˜
NUM_WORKERS  = 8    # DataLoader ì›Œì»¤ ìˆ˜

# --- ë¼ë²¨ ë§¤í•‘ ---
label_map = {
    "ì¡¸ìŒìš´ì „": 1,
    "ìŒì£¼ìš´ì „": 0,
    "ë¬¼ê±´ì°¾ê¸°": 0,
    "í†µí™”":     0,
    "íœ´ëŒ€í°ì¡°ì‘": 0,
    "ì°¨ëŸ‰ì œì–´": 0,
    "ìš´ì „ìí­í–‰": 0
}

# --- í–‰ë™ ë²¡í„° ---
action_vocab = ["í•˜í’ˆ","ê¾¸ë²…ê¾¸ë²…ì¡¸ë‹¤","ëˆˆë¹„ë¹„ê¸°","ëˆˆê¹œë¹¡ì´ê¸°","ì „ë°©ì£¼ì‹œ","ìš´ì „í•˜ë‹¤","ê¸°íƒ€"]
action2idx   = {a:i for i,a in enumerate(action_vocab)}
ACTION_DIM   = len(action_vocab)

# --- ì „ì²˜ë¦¬ ì •ì˜ ---
basic_transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])
aug_transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.RandomHorizontalFlip(0.5),
    transforms.RandomRotation(10),
    transforms.ColorJitter(0.2,0.2,0.2),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])
train_transform = aug_transform
val_transform   = basic_transform

# --- Dataset ì •ì˜ (ì–¼êµ´ í¬ë¡­, ê¹¨ì§„ ì´ë¯¸ì§€ëŠ” __getitem__ì—ì„œ None ë¦¬í„´) ---
class SequenceDataset(Dataset):
    def __init__(self, json_root, img_root, label_map, transform, use_face=True):
        self.samples   = []
        self.transform = transform
        self.label_map = label_map
        self.use_face  = use_face

        files = glob(os.path.join(json_root, '**', '*.json'), recursive=True)
        for j in files:
            try:
                data = json.load(open(j, 'r', encoding='utf-8'))
                cat  = data.get('scene_info',{}).get('category_name')
                if cat not in label_map: 
                    continue

                frames = data.get('scene',{}).get('data',[])
                if len(frames) != SEQ_LEN:
                    continue

                base  = os.path.relpath(os.path.dirname(j), json_root)\
                            .replace('label','').rstrip('/\\')
                paths, boxes, ok = [], [], True
                for fr in frames:
                    p = os.path.join(img_root, base, 'img', fr['img_name'])
                    if not os.path.exists(p):
                        ok = False
                        break
                    occ = fr.get('occupant',[])
                    if not occ:
                        ok = False
                        break
                    bbox = occ[0].get('face_b_box')
                    if not bbox or len(bbox)!=4:
                        ok = False
                        break
                    paths.append(p)
                    boxes.append(bbox)

                if ok:
                    self.samples.append((paths, boxes, label_map[cat]))
            except Exception:
                # JSON íŒŒì‹± ì˜¤ë¥˜ ë“±ì€ ë¬´ì‹œ
                continue

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        paths, boxes, label = self.samples[idx]
        seq = []
        for p, box in zip(paths, boxes):
            try:
                x,y,w,h = box
                img = Image.open(p).convert('RGB')\
                         .crop((x, y, x+w, y+h))
                seq.append(self.transform(img))
            except (UnidentifiedImageError, Exception):
                # ê¹¨ì§„ ì´ë¯¸ì§€ë©´ ì´ ì‹œí€€ìŠ¤ ì „ì²´ë¥¼ ë¬´ì‹œ
                return None
        return torch.stack(seq), label

# â­ï¸ ì›ë³¸ CNNEncoder í´ë˜ìŠ¤ë¥¼ ê·¸ëŒ€ë¡œ import/ì •ì˜í•˜ì„¸ìš”
class CNNEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(3, 16, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(), nn.AdaptiveAvgPool2d((1,1))
        )

    def forward(self, x):
        x = self.cnn(x)
        return x.view(x.size(0), -1)

# âœ… TeacherModel ì—ì„œ encoderë¥¼ CNNEncoder() ë¡œ ë°”ê¿‰ë‹ˆë‹¤
class TeacherModel(nn.Module):
    def __init__(self, action_dim):
        super().__init__()
        self.encoder = CNNEncoder()    # â† ë³€ê²½ëœ ë¶€ë¶„
        self.lstm = nn.LSTM(64, 128, batch_first=True)
        self.fc = nn.Linear(128 + action_dim, NUM_CLASSES)

    def forward(self, x, a):
        B, T, C, H, W = x.shape
        x = x.view(B * T, C, H, W)
        feats = self.encoder(x).view(B, T, -1)
        out, _ = self.lstm(feats)
        last = out[:, -1, :]
        return self.fc(torch.cat([last, a], dim=1))

# --- Student ëª¨ë¸ (CNN + LSTM êµ¬ì¡°) ---
class StudentModel(nn.Module):
    def __init__(self):
        super().__init__()
        # Teacherì™€ ë™ì¼í•œ CNN ì¸ì½”ë”
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 16, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(), nn.AdaptiveAvgPool2d((1,1))
        )
        # LSTM
        self.lstm = nn.LSTM(input_size=64, hidden_size=128, batch_first=True)
        # ìµœì¢… ë¶„ë¥˜ê¸° (action vector ì—†ì´ 128-dim â†’ 2 classes)
        self.fc = nn.Linear(128, NUM_CLASSES)

    def forward(self, x):
        # x: [B, T, C, H, W]
        B, T, C, H, W = x.shape
        # (B*T, C, H, W)ë¡œ í¼ì¹˜ê³  CNN ì¸ì½”ë”©
        x = x.view(B * T, C, H, W)
        feats = self.encoder(x).view(B, T, -1)   # (B, T, 64)
        # LSTM
        out, _ = self.lstm(feats)               # (B, T, 128)
        last = out[:, -1, :]                    # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…
        return self.fc(last)                    # (B, NUM_CLASSES)

# --- ë©”ì¸ ì‹¤í–‰ë¶€ ---
if __name__=='__main__':
    # 1) ë°ì´í„° ì¤€ë¹„ & í†µê³„
    json_root = '/home/sienna/sienna_data/json'
    img_root  = '/home/sienna/sienna_data/image'

    full_ds = SequenceDataset(json_root, img_root, label_map, train_transform)
    print(f"âœ… ì „ì²´ ë°ì´í„°ì…‹ ê¸¸ì´: {len(full_ds)}")

    # ê¹¨ì§„ ìƒ˜í”Œ ê±´ë„ˆë›¸ ì¸ë±ìŠ¤
    valid_idxs = []
    labels = []
    for i in range(len(full_ds)):
        item = full_ds[i]
        if item is None:
            continue
        valid_idxs.append(i)
        _, lbl = item
        labels.append(lbl)

    filtered_ds = Subset(full_ds, valid_idxs)
    counter = Counter(labels)
    print(f"ğŸ“Š ë¼ë²¨ ë¶„í¬: {counter}")

    # 2) Train/Val split
    n_train = int(0.8 * len(filtered_ds))
    n_val   = len(filtered_ds) - n_train
    train_ds, val_ds = random_split(filtered_ds, [n_train, n_val])

    # 3) ë¶ˆê· í˜• ë³´ì •ìš© sampler
    train_labels = [labels[i] for i in train_ds.indices]
    weights = [1.0 / counter[l] for l in train_labels]
    sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)

    # 4) DataLoader (I/O ë³‘ëª© í•´ì†Œ)
    train_loader = DataLoader(
        train_ds,
        batch_size=BATCH_SIZE,
        sampler=sampler,
        num_workers=NUM_WORKERS,
        pin_memory=True,
        collate_fn=lambda b: default_collate([x for x in b if x is not None])
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=NUM_WORKERS,
        pin_memory=True,
        collate_fn=lambda b: default_collate([x for x in b if x is not None])
    )

    # 5) Teacher ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    TEACHER_PATH = '/home/sienna/my_ws/code/cnn_lstm_drowsiness_1.pth'
    teacher = TeacherModel(ACTION_DIM).to(device).eval()
    teacher.load_state_dict(torch.load(TEACHER_PATH, map_location=device))
    for p in teacher.parameters():
        p.requires_grad = False

    # 6) Student í•™ìŠµ ì¤€ë¹„
    student = StudentModel().to(device)
    ce_loss = nn.CrossEntropyLoss()
    kd_loss = nn.KLDivLoss(reduction='batchmean')
    optimizer = optim.Adam(student.parameters(), lr=LR)

    def evaluate(model, loader):
        model.eval()
        correct, total = 0, 0
        with torch.no_grad():
            for x, y in loader:
                x, y = x.to(device), y.to(device)
                preds = torch.argmax(model(x), dim=1)
                correct += (preds == y).sum().item()
                total   += y.size(0)
        return correct / total if total else 0

    # 7) Distillation í•™ìŠµ ë£¨í”„
    best_acc = 0.0
    for epoch in range(1, EPOCHS+1):
        student.train()
        sum_loss, cnt = 0.0, 0
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            a = torch.zeros(x.size(0), ACTION_DIM, device=device)

            with torch.no_grad():
                t_logits = teacher(x, a)
            s_logits = student(x)

            t_soft = nn.functional.softmax(t_logits / TEMPERATURE, dim=1)
            s_logp = nn.functional.log_softmax(s_logits / TEMPERATURE, dim=1)

            loss_kd = kd_loss(s_logp, t_soft) * (TEMPERATURE**2)
            loss_ce = ce_loss(s_logits, y)
            loss    = ALPHA * loss_kd + (1-ALPHA) * loss_ce

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            sum_loss += loss.item()
            cnt      += 1

        val_acc = evaluate(student, val_loader)
        print(f"[{epoch}/{EPOCHS}] Loss: {sum_loss/cnt:.4f} | Val Acc: {val_acc:.4f}")

        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(student.state_dict(), 'student_best_cnn_lstm.pth')
            print(f"ğŸ’¾ Best model saved @ epoch={epoch}, acc={best_acc:.4f}")

    print(f"\nğŸ¯ ìµœì¢… ê²€ì¦ ì •í™•ë„: {best_acc:.4f}")
