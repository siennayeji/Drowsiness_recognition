# -*- coding: utf-8 -*-

import cv2
import torch
import torch.nn.functional as F
import numpy as np
from torchvision import transforms, models

# âœ… ê°ì • ë ˆì´ë¸” ì •ì˜
EMOTION_LABELS = {
    0: "ê¸°ì¨",
    1: "ë¶„ë…¸",
    2: "ë†€ëŒ",
    3: "ìŠ¬í””",
    4: "ì¤‘ë¦½"
}

# âœ… ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.resnet18(pretrained=False)
model.fc = torch.nn.Linear(model.fc.in_features, 5)  # í´ë˜ìŠ¤ ìˆ˜ 5ê°œ
model.load_state_dict(torch.load("emotion_recognition_model_small.pth", map_location=device))
model = model.to(device)
model.eval()

# âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì •ì˜
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

# âœ… ì›¹ìº  ì—´ê¸°
cap = cv2.VideoCapture(0)  # 0ë²ˆ ì¹´ë©”ë¼

if not cap.isOpened():
    print("ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

print("ğŸŸ¢ ì‹¤ì‹œê°„ ê°ì • ì¸ì‹ ì‹œì‘!")

while True:
    ret, frame = cap.read()
    if not ret:
        print("í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        break

    # í”„ë ˆì„ ë³µì‚¬ë³¸ ë§Œë“¤ê¸° (ì¶œë ¥ìš©)
    output_frame = frame.copy()

    # BGR â†’ RGB ë³€í™˜
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # ì „ì²˜ë¦¬
    input_tensor = transform(rgb_frame)
    input_tensor = input_tensor.unsqueeze(0).to(device)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€

    # ì˜ˆì¸¡
    with torch.no_grad():
        outputs = model(input_tensor)
        probs = F.softmax(outputs, dim=1)
        pred = torch.argmax(probs, dim=1).item()
        emotion = EMOTION_LABELS[pred]
        confidence = probs[0][pred].item()

    # ê²°ê³¼ í‘œì‹œ
    label = f"{emotion} ({confidence*100:.1f}%)"
    cv2.putText(output_frame, label, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.imshow('Emotion Recognition', output_frame)

    # 'q'ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# âœ… ì •ë¦¬
cap.release()
cv2.destroyAllWindows()
